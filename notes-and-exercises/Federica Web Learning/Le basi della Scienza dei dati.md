# Le basi della Scienza dei dati

Descrizione del corso

La Scienza dei Dati è una scienza nuova; essa è basata sul grande sviluppo dei dati digitali, e sul loro utilizzo sempre più pervasivo e a volte intrusivo nella vita delle persone e nelle attività delle aziende e pubbliche amministrazioni. I dati digitali sono utilizzati nella ricerca e nelle attività produttive, per nuove scoperte scientifiche, per prendere decisioni più efficaci e per poter predire o interpretare fenomeni. Alla base della scienza dei dati sono la statistica e la informatica, che forniscono le tecniche e le tecnologie per elaborare e analizzare i dati nel loro ciclo di vita, e più in generale le scienze sociali, che studiano l’effetto dei dati e tecnologie digitali sulla comunicazione e sulla qualità della vita, l’economia digitale, le cui leggi innovano profondamente rispetto alla economia dei beni e dei servizi, le scienze giuridiche, che investigano le regole per applicare e per difendersi dalle nuove tecnologie, e l’etica, che fornisce gli strumenti per un corretto uso dei dati.

## Lezione 1. Cosa è la Scienza dei Dati

### Unit 1 - Cosa è la Scienza dei Dati

Grandi quantità di dati vengono trasmesse, comunicate, analizzate. Nel 1854 a Londra, epidemia di colera, un medico cercò di capirne i motivi e raccolse dati sulle strade e sulle pompe d'acqua, riuscendo a capirne le cause. è il primo esempio di raccolta di grandi quantità di dati per risolvere un problema.

![image](https://user-images.githubusercontent.com/75806093/132475891-6844f87f-f5c1-4828-bf76-fb199ffdcb87.png)

la famosa mappa disegnata da Snow dell’area di Broad Street nell’anno 1854, in occasione dell' epidemia di peste a Londra. Ogni quadratino nero è un decesso dovuto al colera, i circoletti rossi e blu rappresentano le aree attorno alle pompe dell’acqua potabile erogata dalle diverse compagnie.

![image](https://user-images.githubusercontent.com/75806093/132475668-d8376866-d599-4807-86b6-3cc354fdf76e.png)

I dati digitali sono i dati prodotti e scambiati sulla rete Internet, nelle reti sociali, e in generale in ogni sistema che rappresenti originariamente la informazione mediante fenomeni fisici che adottano un alfabeto binario.

L’insieme di tecniche, metodi, modelli, tecnologie, linguaggi programmativi, apparati giuridici, nuove leggi dell'economia digitale, norme etiche da seguire nell'uso dei dati digitali, viene oramai in tutto il mondo chiamato Scienza dei Dati.

Diverse definizioni sono state date del termine Big data. La definizione del McKinsey Global Institute è: "Un sistema di Big Data si riferisce a dataset la cui taglia/volume è talmente grande che eccede la capacità dei sistemi di basi di dati tradizionali di catturare, immagazzinare, gestire e analizzare". 

- Ogni fenomeno che noi osserviamo ha una nascita, una vita, una maturità e una decadenza. Così pure accade per i dati digitali, che sono caratterizzati nella nostra epoca da un ciclo di vita con una nascita, quando vengono acquisiti dal mondo fisico, una vita, in cui li osserviamo ed elaboriamo, una maturità, quando li usiamo, e infine una decadenza quando non ci servono più.

Il Data Scientist, per essere in grado di condurre un progetto di analisi dati deve conoscere a fondo le tecniche e i modelli della Statistica, dell'Informatica e le tecnologie e i linguaggi della Informatica.

---

### Unit 2 - I big data

L’etimologia del termine “dato” (BORGMAN, 2015) corrisponde al participio passato del verbo dare, “ciò che è dato”. Questo riferimento al passato, come vedremo nel seguito, definisce un limite all’orizzonte culturale della datacy.

Nel mondo sensibile, la parte da noi percepita attraverso i nostri sensi e la nostra mente, e quella percepita per il tramite dei dati digitali, si intersecano e si modificano continuamente, a favore della seconda, dando luogo a ciò che Floridi chiama infosfera (FLORIDI, 2017), vedi anche (BARICCO, 2018), e che nel seguito chiameremo l’ecosistema dei dati.

![image](https://user-images.githubusercontent.com/75806093/132477731-5accf571-a4c8-4ae6-bcea-4d975987b119.png)

Flussi di dati tra PA in Italia, immagine di Bruno de Finetti del 1962

![image](https://user-images.githubusercontent.com/75806093/132477848-6fde05c2-d60f-4afb-a614-08992aa2778d.png)

Un dato è una discriminazione tra stati fisici delle cose (ad esempio il colore di una maglietta è “rosso”, oppure “bianco”) che può, o meno, comunicare un'informazione a un agente umano. Il fatto che porti o non porti un'informazione dipende dalla conoscenza pregressa accumulata e organizzata dall’agente umano.

Dati, informazioni, conoscenza sono tre forme diverse degli stessi artefatti:

- I dati sono rappresentazioni a cui non è in genere associato un significato
- Le informazioni si ottengono applicando ai dati la nostra conoscenza pregressa sui fenomeni osservati 
- La conoscenza è ciò che noi estraiamo dalla informazione mediante elaborazioni o ragionamenti

Le principali caratteristiche secondo cui evolvono i dati digitali sono (vedi diversi esempi nella figura seguente tratta dal sito GO-GLOBE):

    Il grande volume disponibile, che permette di rappresentare in maniera molto più ricca che nel passato il mondo, gli eventi, i fenomeni, l’ambiente, le persone. La grande velocità, che permette di rappresentare la dinamica degli eventi del mondo.
    L'eterogeneità dei tipi di dati, dalle tabelle, ai grafi, ai messaggi scambiati su Twitter e sulle reti sociali espressi in linguaggio naturale, le immagini su Instagram o i video prodotti da un sistema di sorveglianza.

Il termine Big data fa dunque riferimento genericamente all'insieme crescente per volume, velocità e varietà dei dataset digitali disponibili e alle tecnologie che si stanno sviluppando per la loro gestione. Nelle figure seguenti mostriamo lo spazio secondo cui evolve questa crescita, costituito da tre coordinate, e diversi esempi di evoluzione:

    L’ampiezza della realtà osservata (ad esempio il numero di persone nel mondo per cui è stato prodotto il sequenziamento del genoma umano).
    La profondità nella conoscenza della realtà osservata (ad esempio la grande crescita nella conoscenza disponibile su ogni singolo essere umano che il sequenziamento del genoma sta generando recentemente).
    Il tempo (ad esempio la variazione della quotazione delle azioni in Borsa, misurata ogni centesimo di secondo).
    
![image](https://user-images.githubusercontent.com/75806093/132478340-a7aac14f-c317-4f18-a09a-e476bec54366.png)

![image](https://user-images.githubusercontent.com/75806093/132478366-274180a2-99d4-4d9b-ab36-d451422eb79e.png)

![image](https://user-images.githubusercontent.com/75806093/132478404-d1be5330-9662-4584-bb12-4976a16737c7.png)

I dati, rispetto ai beni e ai servizi, hanno caratteristiche proprie che possiamo così riassumere:

- I dati non hanno una fisicità come i beni, e quindi non sono deperibili, anzi, il loro valore aumenta se sono riutilizzati.
- I dati, pur essendo, come i servizi, «intangibili», si possono riprodurre e riutilizzare all’infinito, mentre un servizio svanisce se nessuno lo utilizza
- Anche i dati hanno un ciclo di vita

Come viene generato questo flusso sempre più grande di dati digitali a nostra disposizione? Viene generato dallo sviluppo tecnologico, e in particolare da quattro grandi tecnologie (vedi figura nella pagina successiva) che alimentano la quinta a cui siamo interessati in questo corso, i big data. Esse sono:

    I social media, utilizzati da miliardi di esseri umani, perché sono dotati di una interfaccia semplice da usare e limitano a pochi simboli (il più famoso è il like) l’alfabeto che occorre apprendere per iniziare ad usarli.
    Il mobile computing, la tecnologia dello smartphone si è diffusa nel mondo in pochi anni. La grande novità rispetto al passato consiste nel fatto che siamo connessi sempre e ovunque, purché vi sia una rete che riusciamo a raggiungere.
    L’internet delle cose, si stima che nel 2025 per ogni essere umano vi saranno circa 1.000 sensori nel mondo. L’internet delle cose consiste nella vastissima rete di sensori e attuatori, ad esempio i chip RFID (Radio Frequency IDentification) associati ad oggetti del mondo virtuale e fisico.
    Il cloud computing, che rende facilmente accessibili e condivise grandi risorse di calcolo, permettendo di memorizzare i dati digitali e di conservarli nel tempo.

![image](https://user-images.githubusercontent.com/75806093/132477242-e62e9b49-56f1-4845-a4ee-5ee7a3390cdd.png)

---

### Unit 3 - Il Ciclo di vita del Dato

Nel passato, diciamo dagli anni 50 agli anni 90 del secolo scorso, i dati venivano rappresentati nei sistemi informativi delle organizzazioni sotto forma di liste, e poi sotto forma di tabelle. Un insieme di tabelle utilizzato da un insieme di programmi informatici è anche chiamato base di dati.

OLTP - On line Transaction Processing

![image](https://user-images.githubusercontent.com/75806093/132480802-e1365747-9e76-48bf-b012-6a9c58fb6130.png)

accanto all’OLTP, le organizzazioni hanno sempre avuto bisogno di un secondo tipo di elaborazioni, chiamate On Line Analytical Processing (OLAP)
 
Il ciclo di vita dei dati elaborati a fini di OLAP nei sistemi informativi classici è composto da tre fasi, chiamate sinteticamente ETL:

    una fase di estrazione dei dati (Extract) dalle basi di dati utilizzate dai processi operativi, cioè le tre basi di dati Biglietti, Viaggi, Reclami;
    una fase di trasformazione (Transform) e integrazione dei dati (per creare ad esempio un'unica tabella);
    una fase di caricamento (Load) dei dati nel sistema di analisi OLAP.
    
![image](https://user-images.githubusercontent.com/75806093/132480956-7dea614f-a6db-4bc3-80e1-ca821a20f8c0.png)

Nel nuovo mondo dei big data, le fonti di dati sono un numero infinitamente più grande che nel passato, e sono caratterizzate da più elevato Volume, Velocità, Varietà. Inoltre, avendo a disposizione più dati, noi cerchiamo di utilizzarli per aumentare il loro Valore. Per queste ragioni, il ciclo di vita dei dati digitali diventa molto più articolato, e si compone delle seguenti fasi (che verranno sviluppate nel seguito):

FASE 0 - Formulazione del problema: si definisce il problema da affrontare (es. prendere una decisione, predire un evento, interpretare un fenomeno).

FASE 1 - Scelta delle fonti e acquisizione: si individuano i data set che permettono di risolvere il problema. Un data set è un insieme di dati aventi significato omogeneo.

FASE 2 - Gestione (o preparazione): i dati sono analizzati per migliorarne l'accuratezza (Valutazione e miglioramento della qualità), per renderne più comprensibile il significato (Modellazione, Trasformazione, Arricchimento semantico), per «metterli insieme» (Integrazione), per ridurne la dimensione.

FASE 3 - Analisi: i dati sono analizzati utilizzando i metodi della Statistica classica ovvero mediante tecniche di Machine learning.

FASE 4 - Visualizzazione dei risultati: i dati sono rappresentati mediante disegni e diagrammi, per rendere più chiari e direttamente percepibili anche ai non addetti ai lavori le conclusioni dell'analisi. La visualizzazione può essere anche utile prima dell'analisi, in cui si osservano i dati per capire quali ulteriori attività di preparazione oppure quali attività di analisi vanno effettuate.

![image](https://user-images.githubusercontent.com/75806093/132481237-b316a79a-faac-4952-ac79-9c5ec1b358cc.png)

La premessa al ciclo di vita del dato digitale è la formulazione del problema; tale problema può consistere per esempio nel:

    prendere una decisione,
    comprendere le ragioni di un evento (o un fatto),
    classificare un insieme di eventi,
    predire un evento futuro.
    
La prima fase del ciclo di vita dei dati digitali consiste nella scelta delle fonti che forniscono dati corrispondenti alle variabili in input, e nella loro raccolta.

I dati vanno a questo punto raccolti, memorizzati e ‘osservati’, per prepararli alla successiva analisi.

Se i dati sono caratterizzati da un flusso continuo, come negli streaming data, vanno filtrati, memorizzati e trasformati in serie temporali, per essere elaborati solo successivamente. In alcune applicazioni, ad es. le quotazioni di borsa, i dati vanno elaborati in tempo reale. Nelle reti di sensori, i dati vanno consolidati in un unico luogo fisico e logico. Poiché i dati saranno sottoposti a molte elaborazioni prima di essere utilizzati, è utile associare ai dati i cosiddetti metadati, cioè dati sui dati. Esempi di metadati sono:

    L’owner, cioè il soggetto, umano o tecnologia, che ha creato i dati. Questo metadato è utile perché se abbiamo, ad esempio, problemi di interpretazione del dato possiamo risalire all’owner.
    Nel caso di un dataset che sia già stato sottoposto a elaborazioni, è utile conoscere la fonte di provenienza, cioè, se è stato conservato, il data set iniziale.
    La data di acquisizione, cioè la data in cui il dataset è stato estratto.
    Il livello di sicurezza negli accessi, cioè i soggetti che possono leggere il dato, modificare il dato, aggiornare il dato.
    
Problemi che possono sorgere nella Fase 1:

- Incompletezza delle fonti; in questo caso dobbiamo cambiare strategia, cioè dobbiamo cercare altri dati, diversi dai precedenti, per costruire un nuovo modello “variabili di input – variabile di output”, ovvero definire un nuovo problema che approssima il precedente.
- Eterogeneità delle fonti; per poter risolvere le eterogeneità tra i tre record può essere utile avere delle informazioni sulla provenienza, su chi cioè ha generato la prima volta il dato, e sul processo di elaborazione del dato.

---

## Lezione 2. La gestione del dato

### Unit 1 - La Gestione del Dato e il suo Valore

Siamo giunti alla Fase 2 del ciclo di vita del dato, la fase di gestione.

La fase di gestione é usualmente la fase più dispendiosa del ciclo di vita, e può corrispondere fino all’80 % dello sforzo complessivo.

In questa fase, in particolare, occorre governare i problemi connessi alla qualità, varietà ed eterogeneità dei dataset acquisiti in precedenza, che possono avere la forma di tabelle, grafi, mappe geografiche, testi in formato libero, testi strutturati, immagini e video (si parla in questo caso di eterogeneità di tipo del dato) ovvero, anche se i dataset sono dello stesso tipo, possono rappresentare il significato, o, come diremo, la semantica della realtà in diverso modo. Ad esempio una data può essere rappresentata con un solo valore, ovvero con tre valori relativi al giorno, mese e anno: questa è una eterogeneità di significato.

La fase di gestione è composta di cinque passi:

    Modellazione dei dati, in cui i dati vengono rappresentati per mezzo di un modello (ad esempio, nel modello relazionale per mezzo di tabelle);
    Valutazione e miglioramento (Controllo) della qualità, in cui dobbiamo verificare che i dati rappresentino in maniera fedele il fenomeno osservato;
    Integrazione, in cui vogliamo «fondere» i diversi dataset che fanno riferimento agli stessi fenomeni, eventi, persone e che abbiamo raccolto in precedenza, in termini di un unico dataset;
    Trasformazione, in cui modifichiamo eventualmente il formato e il modello dei dataset (ad esempio, una data invece che con un valore viene rappresentata con tre valori, ovvero una tabella viene frammentata in due tabelle);
    Arricchimento semantico, in cui cerchiamo di accrescere la conoscenza disponibile sui dati 
    
![image](https://user-images.githubusercontent.com/75806093/132488666-ed1c5c89-9604-4352-912d-2cf3aff431ce.png)

Si noti che mentre i passi di Modellazione e di Controllo di qualità sono in genere eseguiti all’inizio della fase di Gestione, gli altri tre passi possono essere eseguiti in diversi ordini.

Nella figura che segue mostriamo i due percorsi che tratteremo nel seguito:

    nel primo, assumendo come input due tabelle, noi eseguiamo la loro integrazione, producendo come output la tabella che individua e fonde i record delle due tabelle che fanno riferimento allo stesso oggetto del mondo reale;
    Nel secondo, prima di procedere alla integrazione, trasformiamo le due tabelle dal modello relazionale in un modello più ricco semanticamente (vedi in seguito), poi procediamo all'arricchimento semantico e infine procediamo alla integrazione. Vedremo che l’arricchimento semantico può migliorare la qualità del processo di integrazione.

Una caratteristica comune a tutti i passi della fase di gestione dei dati è l’arricchimento del loro valore, o Valorizzazione. 

![image](https://user-images.githubusercontent.com/75806093/132488826-34ffc6eb-bc08-47f5-95d3-3e96928ec890.png)

Cosa significa affermare che i dati hanno un valore? Il valore di un dato può assumere diverse forme:

    Valore conoscitivo, quando accresce la nostra conoscenza su un fenomeno;
    Valore decisionale, quando ci permette di prendere una decisione che valutiamo utile;
    Valore sociale, quando la disponibilità del dato migliora la nostra vita, negli aspetti riguardanti la salute, la sicurezza, la cultura;
    Valore economico, quando la disponibilità del dato si tramuta in un guadagno economico.
    
- Riguardo al valore economico, accanto alle risorse umane, le risorse logistiche, la risorsa dati può essere considerata un importante asset di una organizzazione;
- un asset è definito come ogni risorsa tangibile o intangibile che ha la potenzialità di essere posseduto o gestito per produrre valore, e che è mantenuto per generare valore economico. Si pensi, ad esempio, all'anagrafe dei clienti di una compagnia telefonica;
- mentre l’hardware (i computer) e il software (i programmi) sono asset cui usualmente viene associato un valore economico, la valorizzazione economica dei dati (es. quanto valgono le informazioni disponibili sui clienti) è spesso ignorata nelle teorie economiche, sebbene intuitivamente i dati siano l’asset potenzialmente a maggior valore di una organizzazione;
- come altri asset economici, i dati hanno un costo di produzione e un valore economico (pensiamo ancora ai dati sui clienti, ad esempio per Amazon);
- in ogni caso, i dati hanno proprietà e leggi uniche che li caratterizzano rispetto ai beni e ai servizi, e che li legano qualitativamente al valore.

Il tipico prodotto economico ha le seguenti proprietà:

- Appropriabilità 
- Valore, che diminuisce con l’uso
- Scarsità

I dati digitali rispettano leggi significativamente diverse. I dati digitali invece:

- Sono infinitamente condivisibili (ad es. gli open data sul Web, possono essere acceduti da un numero di persone e applicazioni vastissimo, senza conseguente perdita di valore);
- Hanno un valore che cresce con l'accuratezza, perché dati più accurati hanno maggiore valore decisionale;
- Hanno un valore che cresce quando sono combinati con altri dati, perché in questo modo posso correlare più proprietà;
- Hanno un valore che cresce e non diminuisce con l’uso;
- Come i beni e i servizi, hanno un ciclo di vita, per cui dopo un certo tempo esauriscono la loro utilità e il valore.

Information overload

Herbert Simon ha introdotto il concetto di razionalità limitata, concetto per cui quando analizziamo un problema, la nostra razionalità è ridotta dalle limitazioni cognitive della nostra mente e dal tempo disponibile per trattare il problema;

Nel fenomeno dell’ information overload (sovraccarico di informazione) accade che nell’acquisire nuovi dati su un fenomeno, cresce inizialmente il valore della conoscenza (o valore conoscitivo) a noi disponibile; quando, in virtù della nostra razionalità limitata, non riusciamo più a comprendere nella sua complessità ed estensione la conoscenza disponibile, nuovi dati ci mandano in confusione, occultando e facendo svanire quanto elaborato in precedenza;

Per contrastare il fenomeno dell’information overload dobbiamo procedere all'eliminazione dei dati non rilevanti; ciò può farsi applicando tecniche di filtraggio, ovvero trasformando i dati e producendo di essi versioni più astratte.

![image](https://user-images.githubusercontent.com/75806093/132489896-ab99efad-32e4-4010-b652-b4c97249634b.png)

La figura mostra in forma qualitativa come evolve il valore conoscitivo all'aumentare dei dati disponibili. All'inizio, più dati corrispondono a più valore (vedi prima cornice rossa). Ma da un certo punto in poi i nuovi dati a noi disponibili sono così tanti che non riusciamo cognitivamente a considerarli insieme agli altri per produrre nuova conoscenza (questo è il punto di massimo valore).

Da questo momento in poi (terza cornice), nuovi dati non solo non riescono a produrre nuova conoscenza, ma provocano un fenomeno di “blocco”, per cui portano a una sorta di regressione e di cancellazione della conoscenza accumulata, ovvero una situazione di tale confusione che precedenti certezze o conclusioni tornano in discussione.

Esiste un antidoto all’information overflow? La risposta è sì: possiamo usare tecniche di filtraggio, che selezionano i soli dati che rispettano una certa condizione, ovvero possiamo passare a una versione dei dati più sintetica, che elimina i dettagli non rilevanti, una versione cioè a un più alto livello di astrazione. 

Con i big data è d’obbligo usare astrazioni, e adattare le rappresentazioni e le tecniche al crescere della complessità.

---

### Unit 2 - Gestione del dato: Modellazione

Per poter comprendere e poter condividere con altri i dati, occorre modellarli, cioè descriverli per mezzo di un insieme di strutture. Introduciamo qui tre modelli: il modello relazionale, usato nelle basi di dati, il modello Entità Relazione, usato nella progettazione di basi di dati, e il modello RDF usato nel Web. 

Il Modello relazionale, adottato nelle basi di dati, rappresenta i dati mediante tabelle.

Le tabelle sono il tipo di dati adottato da molto tempo nei sistemi di gestione di basi di dati tradizionali, e furono introdotte da Ted Codd per offrire all’utente una rappresentazione di “spartana semplicità”, superando le rappresentazioni gerarchiche e collegate dei primi sistemi di gestione di basi di dati. 

Nel modello relazionale ogni tabella ha un nome, un insieme di colonne dette attributi che rappresentano proprietà, e un insieme di righe, dette record, che rappresentano oggetti del mondo reale attraverso valori degli attributi. L’insieme dei nomi delle tabelle e degli attributi (vedi nell'immagine la linea discontinua) è anche chiamato schema della tabella.

![image](https://user-images.githubusercontent.com/75806093/132493428-fe05fe91-4e54-4207-b5fc-72936ac50753.png)

I modelli dei dati sono molto utili e aumentano il valore dei dati perché forniscono una descrizione di alto livello di una base di dati, che elimina tutti gli aspetti di realizzazione fisica. Sono utili anche perché permettono di esprimere le interrogazioni sulla base di dati mediante linguaggi anche essi di alto livello.

![image](https://user-images.githubusercontent.com/75806093/132493671-d001de0c-e836-4245-9482-a4b795a57e91.png)

Le due operazioni fondamentali che si effettuano su una tabella o un insieme di tabelle nelle basi di dati sono dunque:

    Le interrogazioni, come quella della penultima pagina, che selezionano da una tabella o un insieme di tabelle tutti i record che rispettano una data proprietà. Nota che una interrogazione non cambia lo stato della base di dati, piuttosto «estrae» dati dalla base di dati, lasciandola non modificata.

    Le transazioni, che modificano la base di dati, aggiungendo record, modificando record o cancellando record.
    
Nel modello Entità Relazione (attenzione: in questo caso non rappresentiamo i valori, ma solo lo schema), che è dotato di una rappresentazione grafica, le entità (rettangoli) rappresentano classi di eventi, fatti, persone, ad esempio Studenti, Studenti esteri, Corsi.

Le relazioni (rombi) rappresentano legami logici tra entità; ad esempio, Esame rappresenta un legame tra Studenti e Corsi. I pallini rappresentano proprietà delle Entità e Relazioni, dette attributi. La freccia con nome È uno rappresenta un legame di sottoinsieme: gli studenti esteri sono un sottoinsieme degli studenti.

![image](https://user-images.githubusercontent.com/75806093/132493908-8d83f2bc-b969-4bbc-99b7-23170edee8de.png)

Le basi di dati, utilizzate nei sistemi informativi in tutto il mondo, fanno uso del modello relazionale. Una base di dati è un insieme di tabelle. Quindi, per progettare una applicazione software che usa una base di dati, è necessario che la applicazione informatica e il linguaggio adottato facciano riferimento al modello relazionale.

Allo stesso tempo, è più semplice da comprendere uno schema espresso nel modello Entità Relazione, anche per la sua intuitiva rappresentazione grafica.

Nel Web i dati possono essere illimitatamente condivisi, non ci sono gerarchie, tutti gli utenti sono potenzialmente parte di una comunità tra pari. Il fenomeno della condivisione dei dati sul Web porta necessariamente a adottare rappresentazioni dei dati che possano fare riferimento le une alle altre attraverso collegamenti o link, creando così una rete, un arcipelago di dati a partire da dataset isolati. Nel Web i dati non possono più essere collegati attraverso i valori, come accade nel modello relazionale.

Per rappresentare dati nel Web è perciò necessario usare rappresentazioni cosiddette di linked data (dati collegati), o grafi semantici o grafi di conoscenza, che adottano strutture a grafo, le quali permettono di collegare attraverso i rami del grafo dataset pubblicati sul Web da fonti e in momenti diversi. Un modello cosiddetto a grafo semantico è l’RDF.

![image](https://user-images.githubusercontent.com/75806093/132494368-676fdada-9304-4c0e-8438-e802bceddba5.png)

Il modello RDF (Resource Description Framework) (esempio nell'immagine) è basato sulla idea di esprimere proprietà di risorse rappresentate nel Web (dati, pagine, cani, gatti, animali, ecc.) con espressioni della forma soggetto-predicato-oggetto, anche chiamate triple nella terminologia RDF.

In una tripla, il soggetto denota una risorsa, il predicato rappresenta la proprietà della risorsa, espressa attraverso una relazione tra soggetto e oggetto, che può costituire anche esso una risorsa.

A ogni risorsa è associato un Identificatore Universale di Risorsa (URI, Universal Resource Identifier), cioè una sequenza di dati che identifica univocamente una risorsa generica nel Web.

![image](https://user-images.githubusercontent.com/75806093/132494529-d6845441-fd22-435d-840a-e8d37097a33b.png)

### Unit 3 - La Gestione del Dato: Qualità

L'inarrestabile crescita dei dati prodotti nel mondo digitale può essere rappresentata come una sfera che si espande nel tempo e nello spazio della realtà osservata.

Nei big data una rappresentazione precisa e circoscritta di un insieme di fatti può trasformarsi (non sempre, per fortuna!) in una rappresentazione opaca e sfocata, affetta da errori.

L’immagine della sfera è anche utile per farci capire che, a seguito dei nostri limiti cognitivi, un ampliamento della conoscenza dei fenomeni può portarci a conoscere frammenti sempre più «spezzettati», eterogenei e imprecisi di una realtà complessa, che vanno ricomposti in una conoscenza comune. Queste riflessioni ci fanno capire quanto sia importante riconciliare i diversi frammenti di conoscenza, e ricostruirne una rappresentazione integrata.

![image](https://user-images.githubusercontent.com/75806093/132504837-7fb7f8a4-683a-4812-8180-58d1a0c63feb.png)

    Qualità - Caratteristica di un artefatto che si basa sulla capacità dell’artefatto di soddisfare necessità ed aspettative esplicite o implicite.
    Dimensione di qualità - Uno specifico aspetto della qualità, usualmente non misurabile.
    Metrica di qualità - Una procedura di misurazione che, partendo dalla dimensione da misurare, associa ad essa un valore numerico (es. 0,7) o ordinale (es. alta) in un dominio di valori, ad esempio [0…1] ovvero [molto bassa, bassa, media, alta, molto alta].
    
Nel passo di valutazione e miglioramento della qualità dei dati si controlla la qualità per mezzo di un insieme di dimensioni di qualità (vedi anche [Batini e Scannapieco, 2016]), tra cui le più importanti sono:

    l’accuratezza, intesa come aderenza del dato al fenomeno osservato;
    la completezza, cioè l’estensione con cui il dato rappresenta la realtà osservata;
    la tempestività di aggiornamento, intesa come rapidità con cui cambiamenti nel fenomeno osservato corrispondono ad aggiornamenti del dato digitale;
    la consistenza, cioè il rispetto di un insieme di regole logiche definite per rappresentare le proprietà del dato.
    
Valutare la qualità del dato è il passo propedeutico a migliorarne la qualità. Per poter operare sul dato al fine di migliorarne la qualità, dobbiamo ampliare la nostra conoscenza attorno al dato.

---

## Lezione 3. Gestione, analisi e visualizzazione dei dati

### Unit 1 - Gestione del Dato: Integrazione

![image](https://user-images.githubusercontent.com/75806093/132692055-9a8b2db0-4540-4c6b-818b-c61ec98c348e.png)

L’attività di confronto dei record delle tre tabelle alla ricerca dei record che corrispondono alla stessa impresa, più in generale allo stesso oggetto del mondo reale, è detta record linkage o anche object matching.

Una volta individuati i record relativi alla stessa impresa, è opportuno poi unirli in un unico record, scegliendo tra valori degli stessi attributi in generale diversi; questa attività è detta fusione.

Come si decide quali siano i record relativi allo stesso oggetto del mondo reale? Presentiamo ora una metodologia per il record linkage, dove per metodologia intendiamo un insieme di attività, basate sull' esperienza, volte al raggiungimento di un obiettivo.

Confrontare (prima attività del passo di integrazione)

Possiamo per esempio calcolare la distanza di Jaccard tra i diversi insiemi di parole che costruiscono i nomi, data da:

1 – numero di parole comuni/numero totale delle parole nel due insiemi

Decidere (seconda attività dell'integrazione)

Fusione (terza attività della integrazione)

Il passo d'integrazione fa crescere il valore dei dati, perché quando i dati relativi a un certo fenomeno o evento sono frammentati abbiamo una visione «compartimentata» del fenomeno, mentre quando sono integrati possiamo conoscere molte più informazioni di prima, perché i dati non sono più separati.

---

### Unit 2 - Gestione del Dato: Trasformazione e Arricchimento semantico

Nei passi di trasformazione e arricchimento semantico noi trasformiamo i dataset in un modello più ricco semanticamente, il modello RDF, che, in virtù della sua struttura a grafo, può sfruttare risorse e tecniche semantiche disponibili nel Web.

![image](https://user-images.githubusercontent.com/75806093/132695658-75312f9e-a885-4dc3-9b80-1e043c1a9230.png)

DBPedia è un ricco contenitore di conoscenza creato e continuamente arricchito da una comunità di utenti che può fare al caso nostro!

---

### Unit 3 - Analisi e Visualizzazione

La grande novità insita nell'avere disponibili tanti dati sta nel superamento dei vincoli delle analisi statistiche classiche, in cui i dati disponibili sono pochi, ed è troppo costoso o impossibile operare su tutto l’universo sotto osservazione, limitandosi a analizzare un campione. Ciò richiede l’uso di tecniche di statistica inferenziale, che permettono di valutare l'adeguatezza del campione a inferire, a partire dalle sue proprietà, corrispondenti proprietà dell’universo.

Oggi di molti fenomeni, e con le infrastrutture di calcolo parallele di recente generazione, è possibile analizzare l’intero universo su cui si effettua l'analisi. Inoltre, il progressivo aumento della dimensione dei dataset permette di estrarre e analizzare molti più dati rispetto a quelli che si possono ottenere analizzando piccoli dataset e serie temporali; ad esempio, l'analisi per sondare gli "umori" dei mercati e del commercio e per effettuare profilazioni di utenti, può essere arricchita analizzando accanto a dati di natura anagrafica e professionale, anche dati geo-referenziati, dati dalle interazioni con i siti ottenuti attraverso canali social (effettuando la cosiddetta sentiment analysis).

Corrispondentemente, si sono evoluti i metodi statistici e, successivamente, sono stati sviluppati metodi basati sul machine learning.

I metodi statistici per trovare la relazione che lega una variabile in output con un insieme di variabili in input, sono basati sui concetti di correlazione e regressione.

In statistica, una correlazione è una relazione tra due variabili tale che a ciascun valore della prima corrisponda, con una certa regolarità, un valore della seconda. Contrariamente a quanto si potrebbe intuire, la correlazione non esprime una relazione di causa effetto quanto un legame più debole, che caratterizza la tendenza di una variabile a cambiare valore in relazione ad un'altra.

Quando si voglia esprimere un legame funzionale tra due variabili, si usano tecniche cosiddette di regressione per individuare la funzione che meglio approssima i dati correlati.

![image](https://user-images.githubusercontent.com/75806093/132697597-f6ce5179-76f5-404e-aa27-f8a1aaf888fa.png)

Il Machine Learning si può definire come:

    l’insieme di metodi per realizzare “un processo mediante il quale un sistema migliora la propria performance” (H. Simon);
    “un processo di acquisizione di conoscenza in assenza di esplicita programmazione” (L.G. Valiant);
    un processo di creazione di un algoritmo o programma che consente di svolgere un compito sulla base di informazioni che non forniscono un’esplicita descrizione di quel programma.
    
Tipi di apprendimento:

Apprendimento supervisionato: all'algoritmo di apprendimento sono forniti, come esperienza, dati di input e rispettivi dati di output ad essi collegati tramite una funzione che associa a ciascun input il corrispondente output; questo processo è anche chiamato classificazione. Tipicamente, come esempi di algoritmi di apprendimento supervisionato vi sono quelli che si basano sugli Alberi di decisione e le Reti Neurali. Una volta che l’algoritmo ha appreso la funzione a partire dai dati (training set), segue una fase di test per valutare la sua capacità di generalizzazione, la capacità, cioè, di fornire output corretti a partire da input non contenuti nel training set.

Apprendimento non supervisionato: all’algoritmo di apprendimento sono forniti, come esperienza, dati di input senza che questi siano associati a valori di output, ed esso ha il compito di riconoscere schemi/strutture nell’insieme dei dati fornito. Rientrano nella classe degli algoritmi di apprendimento non supervisionato quelli di clustering, ossia quegli algoritmi che hanno il compito di raggruppare i dati sulla base di strutture, schemi o affinità che essi devono riconoscere.

Apprendimento per rinforzo: l’algoritmo di apprendimento apprende sulla base della risposta dell’ambiente alle sue azioni; in tal caso, infatti, vi è un’interazione con un ambiente nel quale esso deve svolgere un compito e dal quale ottiene una risposta in termini di “ricompensa” (il rinforzo) che consiste nella valutazione della prestazione. Come esempi, citiamo gli algoritmi che imparano le strategie da usare in un gioco attraverso partite contro un avversario e quelli che consentono ad un robot di muoversi all’interno di un ambiente con un certo scopo.

Per costruire un modello di classificazione, dobbiamo utilizzare una Tecnica di Classificazione (o Classificatore), cioè un approccio sistematico alla costruzione di Modelli di Classificazione da un data set. La tecnica si basa sulla disponibilità di un training data set.

Se si ha disponibile un Dataset di training si parla di tecnica di apprendimento supervisionato. Nell'apprendimento supervisionato il training set è usato per costruire (apprendere e validare) un Modello di Classificazione, che è successivamente applicato al dataset di test.

Una Tecnica di Classificazione è un approccio sistematico alla costruzione di Modelli di Classificazione di un dataset. Lo schema qui sotto fornisce l’approccio generale per risolvere un problema di Classificazione.

Il Modello di classificazione è appreso usando i record dei Training data set. L’apprendimento del modello di classificazione avviene usando la tecnica di apprendimento.

L’induttore (inducer in inglese) è il prodotto della attività di apprendimento del modello di classificazione, ed è una particolare applicazione del modello di classificazione. Esso è invocato per predire il valore dell’attributo predittivo
nel data set di test.

![image](https://user-images.githubusercontent.com/75806093/132698344-11a9a71e-fb99-424e-92c9-a0acfd8d4c88.png)

Lo scopo della visualizzazione è far percepire il contenuto di un insieme di dati mediante rappresentazioni basate sul senso della visione.

Si dice che un’immagine vale più di 1.000 parole. I simboli grafici, le metafore, i colori che possiamo usare per esprimere una idea o, nel nostro caso, il risultato di una analisi sono molto più efficaci di un insieme di parole o valori numerici che l'immagine rende visivamente. Tuttavia, nello scegliere la visualizzazione, occorre stare attenti che non distorca il significato delle parole o dei numeri che intende rappresentare.

Accanto alla Literacy, capacità di comprendere e scrivere testi e esporre un argomento, e alla Numeracy, capacità di esprimere un problema in matematica, si sta affermando la Datacy, la capacità di scegliere i dati che ci servono per un problema, di rappresentarli attraverso modelli, di interrogarli per mezzo di linguaggi, di accrescerne la qualità e arricchirne il significato, di comprenderne il valore, di analizzarli e visualizzarli per risolvere problemi. 

---

## Lezione 4. Concetti introduttivi a R e Python 

### Unit 1 - Espressioni aritmetiche, relazionali, logiche

Confronto introduttivo tra R e Python

R è un ambiente di sviluppo di applicazioni per la analisi e visualizzazione di dati; nasce in ambito statistico come ambiente in cui poter utilizzare la “cassetta degli attrezzi” costituita dalle tecniche statistiche sviluppate nel mondo della ricerca. R è utilizzato soprattutto negli ambiti della formazione e della ricerca; è un software libero in quanto viene distribuito con la licenza GNUGPL, ed è disponibile per diversi sistemi operativi (ad esempio Unix, GNU/Linux, macOS, Microsoft Windows). R è un linguaggio per il quale una ampia comunità di sviluppatori ha condiviso, ispirandosi alla filosofia del software libero, un vasto insieme di package di analisi e visualizzazione, disponibili nel sito CRAN.

Python è un linguaggio di programmazione per la analisi e visualizzazione e utilizza un vasto insieme di strutture di dati; è prevalentemente utilizzato in ambito professionale. La sintassi di Python, insieme al suo carattere di linguaggio interpretato, cioè immediatamente eseguibile, lo rendono adatto allo sviluppo rapido di applicazioni su un ampio insieme di piattaforme. Anche Python è dotato di una ampia libreria di moduli e strumenti di sviluppo; sia l’interprete Python che la libreria sono disponibili come software libero.

R è più semplice da apprendere rispetto a Python, essendo Python un linguaggio di programmazione completo.

 I linguaggi di programmazione, e il nostro linguaggio semplificato, utilizzano le variabili, che sono dati a cui è associato un nome (ad esempio I, J, SOMMA, MEDIA) e a cui nel corso del programma sono assegnati dei valori.

L'istruzione tipica nel nostro linguaggio con cui, ad esempio, possiamo assegnare alla variabile SOMMA il valore 4 + 5 è la istruzione di assegnazione, che scriviamo: SOMMA = 4 + 5

Quando è eseguita questa istruzione, viene prima calcolato il valore di 4 + 5, cioè 9, e successivamente tale valore è assegnato alla variabile SOMMA.

Come vediamo, nella parte sinistra di una istruzione di assegnazione compare sempre una variabile, e nella parte destra possono comparire espressioni aritmetiche, composte in generale di costanti e variabili. Nell’esempio precedente, l'espressione aritmetica è composta da costanti, 4 e 5, sommate tra di loro. Attenzione – Il simbolo = usato nella istruzione di assegnazione ha significato diverso rispetto all’ = che si usa in aritmetica, che esprime una uguaglianza tra termini, ed ha sintassi diversa in R e Python.

Una espressione aritmetica è un insieme di costanti intere o frazionarie (es. 5; 138,4) e variabili unite da operatori aritmetici +, -, x, /, ed eventualmente racchiuse tra parentesi per stabilire l’ordine con cui va eseguito il calcolo dell'espressione.

Una espressione aritmetica che compare in una istruzione di assegnazione ha un valore, che può essere calcolato sostituendo nell'espressione alle variabili il loro valore e calcolando l'espressione.

Le espressioni relazionali hanno un valore, che può essere VERO o FALSO.

La forma sintattica delle espressioni relazionali è:

<espressione aritmetica>operatore di confronto <espressione aritmetica> dove l’operatore di confronto può essere:
    
    = che significa uguale
    /= diverso
    > maggiore
    >= maggiore o uguale
    < minore
    <= minore o uguale

Le parentesi < > attorno a espressione aritmetica hanno il significato di una qualunque. Quindi la forma sintattica vista sopra esprime il fatto che una espressione relazionale è una coppia di espressioni aritmetiche congiunte da un operatore di confronto.
    
Le espressioni logiche sono composte di espressioni relazionali (che, ricordiamo, hanno come valori possibili VERO o FALSO) connesse da operatori logici OR, AND, XOR, NOT. Il loro valore è sempre un valore VERO o FALSO.
    
Il valore di una espressione logica si trova calcolando dapprima i valori delle espressioni aritmetiche e relazionali e successivamente applicando gli operatori logici. I valori degli operatori logici in funzione dei loro operandi, detti anche tabelle di verità, sono riportati nel seguito.
    
L’OR di due espressioni logiche è vero se è vera o l’una o l’altra
    
L’AND di due espressioni logiche è vero se è vera sia l’una sia l’altra

![image](https://user-images.githubusercontent.com/75806093/132714505-2f0e56a9-7139-4fe2-8604-afeb57c5797e.png)

L’ XOR di due espressioni logiche è vero se è vera o l’una o l’altra, ma non tutte e due
    
Il NOT di una espressione logica è vero se la espressione è falsa, falso altrimenti.
    
![image](https://user-images.githubusercontent.com/75806093/132714606-35e8edc2-3ef5-4e2a-80f5-2b58697834db.png)

---
    
### Unit 2 - Strutture di controllo
    
Un programma è una sequenza di istruzioni, che, nel nostro linguaggio semplificato possono essere:

    di assegnazione;
    di lettura o scrittura;
    If Then Else;
    For.

Per distinguere una istruzione dalla successiva useremo il simbolo ;

Per esprimere che il testo di un programma è terminato inseriamo alla fine del programma la istruzione End.
    
La forma generale di una istruzione if Then Else è

If <espressione logica> Then <insieme di istruzioni1> Else <insieme di istruzioni2>

Quando una istruzione If Then Else è eseguita, viene calcolata la espressione logica; se è vera, viene eseguito <insieme di istruzioni1>, altrimenti viene eseguito <insieme di istruzioni2>
    
Il ciclo For è utilizzato per ripetere un certo numero di volte l’esecuzione di una o più operazioni.
    
La forma generale del ciclo For nel nostro linguaggio è:

For <variabile> = <variabile/costante intera> to <variabile/costante intera> <sequenza di istruzioni> EndFor

dove i simboli <…> e / in <variabile/costante intera> devono intendersi: qualunque variabile o costante intera.
    
---
    
### Unit 3 - Strutture di dati
    
![image](https://user-images.githubusercontent.com/75806093/132720038-0ae872ca-06d0-4a2a-9df8-d14cdc824dec.png)

Nella figura sotto la parte costituita dal nome della tabella e dai nomi degli attributi è chiamata Schema, e cambia poco nel tempo. L’insieme delle n-ple è chiamata Istanza, e cambia nel tempo molto frequentemente. Una chiave di una tabella è costituita da uno o più attributi i cui valori compaiono una sola volta nelle n-ple della tabella.

Quindi una chiave, si dice, identifica univocamente le singole n-ple della tabella. 
    
![image](https://user-images.githubusercontent.com/75806093/132720172-8af758eb-7b0c-4227-a822-3cc79aad34e6.png)

Consideriamo la tabella in fondo, che ha come chiave Matricola. Con la notazione

Matricola → Cognome, Nome, Comune di Nascita, Regione di Nascita

detta anche dipendenza funzionale, intendiamo dire che fissato un valore per la Matricola, è unico il valore associato a tutti gli altri attributi.
    
Come possiamo fare per eliminare le ridondanze? Dobbiamo separare in diverse tabelle gli attributi delle due dipendenze funzionali, dando luogo alla decomposizione della tabella Studente in due tabelle, come vediamo qui sotto. Il processo di decomposizione è anche chiamato di normalizzazione.

La nuova base di dati con due tabelle ha lo stesso contenuto informativo della precedente, e è detta essere in forma normale. La tabella di partenza può sempre essere ricostruita dalle due tabelle, collegando le n-ple delle due tabelle che hanno lo stesso valore per il Comune. Questo è il significato delle doppie frecce.
    
![image](https://user-images.githubusercontent.com/75806093/132720502-f0269ecf-67bf-4895-9cf7-3341b38d3ba4.png)

I concetti rilevanti della struttura Tabella sono:

    Nome (della Tabella);
    Attributo;
    N-pla o record;
    Dominio dei valori di un attributo;
    Schema;
    Istanza;
    Chiave;
    Super-chiave;
    Dipendenza funzionale;
    Processo di normalizzazione;
    Forma normale.
    
Accenniamo ad altre due strutture di dati, le matrici e i vettori.

Un esempio di matrice è la Tavola Pitagorica dei primi 5 numeri interi.

![image](https://user-images.githubusercontent.com/75806093/132720597-10d4f1b5-8ca5-4e99-a724-2906101d2dff.png)

Una matrice M di dimensione N x K è costituita da n= 1, 2, .., N righe e k= 1, 2, ..,K colonne. Ogni elemento della matrice M (i,j), caratterizzato da una posizione di riga e una posizione di colonna, rappresenta usualmente valori definiti tutti nello stesso dominio.
    
![image](https://user-images.githubusercontent.com/75806093/132720666-379657cc-e22e-4fff-9529-9f7f9fbafb02.png)

    
Un vettore o array è un insieme ordinato di elementi omogenei, identificati da uno stesso nome (il nome dell’array) e da uno o più indici. Un vettore può anche vedersi come una matrice con una sola colonna. Ad esempio se, da una matrice che rappresenta le coppie di nodi collegati di un grafo di sei nodi, vogliamo calcolare per ogni nodo rappresentato il numero di nodi con cui è collegato, otteniamo l’array nella parte destra.
    
![image](https://user-images.githubusercontent.com/75806093/132720752-191a944c-f7b3-4908-9218-e1736a67f776.png)
